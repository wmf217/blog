---
layout:     post
title:      "kafka"
subtitle:   " \"kafka理论和实践\""
date:       2019-02-08 20:44:00
author:     "wmf"
header-img: "img/in-post/bigdata.jpg"
catalog: true
tags:
    - java
---
## 消息系统
1.点对点模式
2.发布订阅模式
## topic&partition
![](/img/in-post/topic.png)
##### topic
每个发送到kafka的消息都有一个类别，这个类别就是topic(主题)，它是一个抽象概念
##### partition
一个topic是由多个partition(分区)组成，一个partition的消息是有序的，但是多个partition之间是不保证有序的，它是一个物理概念，客户端发布和订阅topic时不需要关心具体从哪个分区读取的数据，当然发布消息时是可以指定分区的
##### offset
kafka支持位移，也就是offset，消费者可以去指定offset下读取数据
![](/img/in-post/offset.png)
## payload
kafka发送消息的内部单元叫payload，以topic来分类，它是一个字节数组，不关心数据的具体格式<br>
每个payload都有一个可选的metadata，被称作key，key同样是一个字节数组<br>
payload被写入partition的是否，key控制分派过程，默认使用hash取模，如果不存在key，使用轮询partition的方式<br>
为了更加高效，payload以批量提交的方式写入kafka，这些payload有相同的topic和partition，它是吞吐和延迟的权衡<br>
*批量提交的触发，一个是达到了预先设置的数据量，一个是达到了预先设置的超时时间*
## Producer&Consumer
Producer生产payload，Consumer订阅一个或多个topic<br>
Consumer自己维护payload的offset<br>
Consumer以Consumer group方式来工作，一个group共同消费一个topic<br>
Consumer group保证topic内的一个partition只会被group中的一个Consumer消费，所以partition一定会被有序消费<br>
group中的consumer出现故障，消费组内的其他消费者会接管它的分区
## kafka的整体架构
![](/img/in-post/kafka1.jpg)
![](/img/in-post/kafka2.jpg)
## kafka的配置
##### broker
broker.id: kafka broker的标识信息<br>
port: 监听端口<br>
zookeeper.connect: zookeeper的连接地址<br>
log.dirs: kafka将消息持久化到磁盘，存在这个目录下<br>
num.recovery.threads.per.data.dir: 数据所对应的目录的线程数(只在启动，恢复数据，关闭时使用，所以可以尽量设大点)<br>
auto.create.topics.enable: 指定的场景下自动创建topic(一般手动设为false，以及时发现没有topic的错误)
###### topic
number.partitions: topic创建时partition个数，默认是1(分区机制可以使得topic可以均衡分布在kafka集群里，很多都把分区数设置成broker的数量，或者是broker数量的整数倍)<br>
**设置分区数的考虑因素？首先是你希望的topic吞吐量是多少，单个分区的吞吐量是多少，比如Consumer从partition中读取数据并处理存入数据库的时间是50mb/s，预期的topic吞吐量是1000mb/s，那至少需要20个分区**<br>
log.segment.bytes: 每个segment的最大数据容量，默认1G(Producer在写入消息时，数据存储在日志段的segment文件中，当segment写满了，就会新建一个segment，旧的segment关闭以后过一段时间就会自动删除)<br>
**如果Producer写入速度很慢，这个参数可以适当的做调整，加入一个topic一天只有100mb数据写入，该参数使用默认值1G，需要10天一个segment才会被写满，如果过期时间为7天，那么此时需要17天才可以删除一个segment**<br>
log.segment.ms: segment强制关闭的时间，没有默认值(一旦设置，到了这个时间segment强制关闭，没有默认值，所以默认使用上面的参数用文件的大小来关闭)<br>
**一个需要注意的点-如果有大量的partition，而这些partition的segment都没有达到文件大小上限，那么当达到log.segment.ms的时间时，这些segement会同时被关闭，大量的IO操作会影响IO性能**<br>
message.max.bytes: payload的最大数据量限制，默认是1mb(当Producer发送的数据超过这个值会发送失败)
## kafka发送消息的基本流程
![](/img/in-post/kafka3.jpg)
## kafka的安装启动
```
tar -zxvf kafka2.3.0.tgz
```
kafka的默认启动需要1G内存，如果计算机内存太小，可以调整
```
free -h // 查看内存剩余
find . -iname '*server-start.sh'
vi /export/server/kafka/bin/zookeeper-server-start.sh
vi /export/server/kafka/bin/kafka-server-start.sh
```
将```-Xms1G -Xms1G```修改为```-Xmx400M -Xms400M```(根据需要，试过了128m跑不起来内存溢出)
启动zookeeper集群
```
bin/zkServer.sh start conf/zoo-1.cfg
bin/zkServer.sh start conf/zoo-2.cfg
bin/zkServer.sh start conf/zoo-3.cfg
```
启动kafka
```
bin/kafka-server-start.sh -daemon config/server.properties
```
在命令后添加-daemon参数，可以使kafka以守护进程方式启动，即不占用窗口<br>
关闭kafka
```
bin/kafka-server-stop.sh config/server.properties
```
关闭后过一会jps可以看到kafka服务已经没有了
## shell操作kafka
创建一个topic
```
kafka-topics.sh --create --zookeeper 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 --partitions 2 --replication-factor 1 --topic wmf
```
或者
```
bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic wmf
```
这两种写法的效果是一样的，只不过第二种直接连接kafka的是新的写法，所以尽量使用```--bootstrap-server```，但老的命令行```--zookeeper```也是支持的，也可以从老的命令行知道topic是在zookeeper中创立的，而kafka对这种topic的创立或者变更一定是做了监听<br>
>The bin/kafka-topics.sh command line tool is now able to connect directly to brokers with --bootstrap-server instead of zookeeper. The old --zookeeper option is still available for now

<font color="red">注意：如果在server.prototypes中listener设置了真实的ip，那么这里使用的就要是真实的ip，而localhost会提示连接不上的错误</font><br>
解释<br>
--partitions 2   #创建2个分区(当前没有通过集群搭建，所以只能创建一个分区)<br>
--replication-factor 1     #复制1份<br>
--topic     #主题为wmf<br>
查看topic信息
```
bin/kafka-topics.sh --describe --bootstrap-server 172.17.201.39:9092 --topic 915-1
```
显示信息
```
Topic:wmf       PartitionCount:2        ReplicationFactor:1     Configs:
Topic: wmf      Partition: 0    Leader: 0       Replicas: 0     Isr: 0
```
查看topic列表
```
bin/kafka-topics.sh --list --bootstrap-server 172.17.201.39:9092
```
删除topic
```
bin/kafka-topics.sh --delete --zookeeper localhost:2181 --topic wmf
```
##### producer发送信息
```
bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic wmf
```
输入信息
```
this is a message from wmf
```
#### consumer接受信息
```
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic wmf --from-beginning
```
获得信息
```
this is a message from wmf
```
即接受到了信息
## 伪集群启动kafka
创建3个配置文件
```
cp server.properties server_1.properties
cp server.properties server_2.properties
cp server.properties server_3.properties
```
修改三个配置文件，这里修改三个参数
```
broker.id=1
listeners=PLAINTEXT://ip:9092
log.dirs=/export/data/kafka-1

broker.id=2
listeners=PLAINTEXT://ip:9093
log.dirs=/export/data/kafka-2

broker.id=3
listeners=PLAINTEXT://ip:9094
log.dirs=/export/data/kafka-3
```
启动三个kafka
```
bin/kafka-server-start.sh config/server_1.properties
bin/kafka-server-start.sh config/server_2.properties
bin/kafka-server-start.sh config/server_3.properties
```
## kafka如何形成集群
由上面的集群搭建可以看出，kafka的配置文件并没有像zookeeper一样在每个配置文件中配置各节点的地址(```server.1=xxx:2181,server.2=xxx:2182,....```)，那么kafka这个集群是怎么形成的，换句话说broker之间如何互相找到彼此(困扰了好久，网上每一个人解释，真是醉了)，实际上这些kafka之所以形成集群是因为他们连接了同一个zookeeper集群，也就是在zookeeper.connect中配置的地址，所有的broker信息都会存储在zookeeper中的/brokers/ids节点中，存储的每一个id的信息大概如下
```
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://172.27.16.2:9092"],"jmx_port":-1,"host":"172.27.16.2","timestamp":"1568370951506","port":9092,"version":4}
```
可以看到每个ids中存储了每一个brokers的地址，那么一个brokers可以通过读取或监听这个节点来知道所有的brokers地址，那么这个集群就形成了，原理图如下
![](/img/in-post/kafka5.png)
## 查看kafka的版本号
kafka没有提供相关命令只能：```cd/libs``` 目录下有一个像kafka_2.12-2.3.0.jar的文件，其中2.3.0就是版本号
## kafka 关于分区的简单理解
![](/img/in-post/kafka4.png)
以上图中所示：创建了一个topic 1的topic，该topic有2个分区1个备份，集群中有三个broker```(kafka-topics.sh --create --zookeeper 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 --partitions 2 --replication-factor 1 --topic topic1)```，为了负载均衡，kafka会尽可能的把每个分区的leader布置在不同的broker上(copy只是一个备份，发布和读取消息时都不会去copy里面读写)，当producer发布消息时，kafka会根据key的hash值或producer自定义的分区规则将消息发往不同的分区。<br>
验证一下，搭建一个三个broker的kafka集群，创建一个topic，两个副本，两个分区
```
bin/kafka-topics.sh --create --zookeeper 129.28.89.201:2181 --partitions 2 --replication-factor 2 --topic wmf-p2-r2
```
此时打卡zookeeper客户端查看信息，首先查看topic:wmf-p2-r2的第一个分区
```
get /brokers/topics/wmf-p2-r2/partitions/0/state
```
出现如下信息
```
{"controller_epoch":44,"leader":0,"version":1,"leader_epoch":0,"isr":[0,1]}
```
可以看到这个分区的leader是0，isr(In-Sync Replicas) list为[0,1]，也就是在1中有一个备份，接着查看第二个分区
```
get /brokers/topics/wmf-p2-r2/partitions/1/state
```
出现如下信息
```
{"controller_epoch":44,"leader":1,"version":1,"leader_epoch":0,"isr":[1,2]}
```
可以看到这个分区的leader是1，isr(In-Sync Replicas) list为[1,2]，也就是在1中有一个备份<br>
一些其它的信息controller_epoch，epoch有纪元、时代的意思，这个数相当于kafka集群controller的版本号，标识当前controller是第几代controller，同样leader_epoch就不难理解了<br>
consumer group是一个consumer组，*一个组的成员共同消费一个topic*，在group1中只有一个消费者，那么它会同时读取两个分区的数据进行消费，而group2中有三个消费者，但<font color="red">一个分区的数据只能被某一个组中一个成员消费</font>，所以只有两个消费者分别实际消费了两个分区的数据，另一个消费者则闲置
## 为什么这么设计
>若没有分区，一个topic对应的消息集在分布式集群服务组中，就会分布不均匀，即可能导致某台服务器A记录当前topic的消息集很多，若此topic的消息压力很大的情况下，服务器A就可能导致压力很大，吞吐也容易导致瓶颈<br>

比如如上的例子，topic1有两个分区，那么客户端我们可以用两个线程建立两个consumer(同一组)，分别去消费两个分区，加快了消费的速度<br>
这里消费这个词很有意思，为什么可以两个线程同时消费，因为这个消费的概念就是无论多少个消费者来处理，<font color="red">消费的逻辑一定是一样的(比如都是读取日志信息存入日志表)，而且不能考虑先后顺序(kafka只能保证同一个分区的消息是有序的，分区之间的消息不保证有序))</font>
## 分区测试
现在创建一个topic 915-1，有两个分区，启动一个producer，
启动两个consumer且在同一分组
```
bin/kafka-console-consumer.sh --bootstrap-server 172.17.201.39:9093 --topic 915-1 --consumer-property group.id=group1
```
上面的group1就是组名。打开两个客户端分别执行如上代码<br>
生产者依次发布两条消息915-1和915-2，效果如下图
![](/img/in-post/kafka7.png)
一个接受到分区0的915-1，一个接受到分区1的915-2，两个加在一起就是所有的message
## Controller
上文提到了controller_epoch代表来了controller的纪元，那么什么是controller<br>
[https://blog.csdn.net/u013256816/article/details/80865540](https://blog.csdn.net/u013256816/article/details/80865540)<br>
上面有一个老的命令行```--zookeeper```可以简单理解controller的一个小作用如下图
![](/img/in-post/kafka6.png)
## bootstrap-server
比如现在我有三个broker，创建一个分区为2的topic 915-1，现在查看topic的状态，显示如下
```
Topic:915-1     PartitionCount:2        ReplicationFactor:2     Configs:segment.bytes=1073741824
        Topic: 915-1    Partition: 0    Leader: 2       Replicas: 2,1   Isr: 2,1
        Topic: 915-1    Partition: 1    Leader: 3       Replicas: 3,2   Isr: 3,2
```
可以看到topic有两个分区，分区的leader分别是2和3
那么此时创建producer和consumer
```
bin/kafka-console-producer.sh --broker-list 172.17.201.39:9092 --topic 915-1
...
bin/kafka-console-consumer.sh --bootstrap-server 172.17.201.39:9092 --topic 915-1 --from-beginning
```
这是无论是producer的broker-list，还是consumer的bootstrap-server都连接的是9092也就是id==1(分别是1:9092,2:9093,3:9094)的broker，也就是这个broker上并没有915-1的消息或者只是副本，但是却正常实现了发布和订阅消息<br>
**所以这个broker-list/bootstrap-server或之前的zookeeper都只是集群中的一个随便的节点，无论连接到哪个节点都是相当于连接整个集群，消息交由controller处理和分配，也就是上面的设为9093,9094都一样的效果，但可以设置多个以免某个宕机**
## 小记录
这里首先记录一下一个困扰两小时的问题，就是pom.xml中的依赖并没有在external libraries中，报错fail to read descriptor for xxx<br>
解决办法：把本地仓库清空重新reimport<br>
配置server.properties时listener的ip配上了确报错
```
kafka.common.KafkaException: Socket server failed to bind to xx:9092: Cannot assign requested address
```
产生原因：虚拟机对外ip(公网ip)[暴露的ip]和真实ip(内网ip)[ifconfig显示的ip]可能只是映射关系<br>
解决办法：查询内网ip```ifconfig -a```, 配置为内网ip
## java
```java
package com.wmf.kfk.Producer;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

import java.util.Properties;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;

/**
 *<h1>kafka producer<h1/>
 * 启动zookeeper
 * 启动kafka
 * 创建topic: kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --partitions 1 --replication-factor 1 --topic wmf
 * kafka producer: kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic wmf
 * kafka consumer: kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic wmf --from-beginning
 * **/
public class Producer {
    // String, String分别是key和value的类型
    private static KafkaProducer<String, String> producer;
    static {
        Properties props = new Properties();
        props.put("bootstrap.servers", "129.28.89.201:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        producer = new KafkaProducer<String, String>(props);
    }

    private static void sendMessageForgetResult() {
        ProducerRecord<String, String> record = new ProducerRecord<String, String>("wmf", "name", "go forget result");
        producer.send(record);
        producer.close(); //kafka是批量提交，为了保证kafka能收到消息。所以执行关闭
    }

    private static void sendMessageSync() throws ExecutionException, InterruptedException {
        ProducerRecord<String, String> record = new ProducerRecord<String, String>("wmf", "name", "go sync");
        RecordMetadata result = producer.send(record).get();
        System.out.println(result.topic());
        System.out.println(result.partition());
        System.out.println(result.offset());
        producer.close(); //kafka是批量提交，为了保证kafka能收到消息。所以执行关闭
    }

    private static void sendMessageAsync() {
        ProducerRecord<String, String> record = new ProducerRecord<String, String>("wmf", "name", "go async");
        producer.send(record, new Callback() {
            public void onCompletion(RecordMetadata recordMetadata, Exception e) {
                if (e!=null) {
                    e.printStackTrace();
                    return;
                }
                System.out.println(recordMetadata.topic());
                System.out.println(recordMetadata.partition());
                System.out.println(recordMetadata.offset());;
            }
        });
        producer.close();
    }

    public static void main(String[] args) throws ExecutionException, InterruptedException {
//        sendMessageForgetResult();
        sendMessageSync();
//        sendMessageAsync();
    }
}

```